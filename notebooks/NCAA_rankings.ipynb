{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing the Ranks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import datetime\n",
    "from functools import reduce\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '/Users/vineethsuhas/vineeth/handsOn/hackathons/kaggle/GoogleMLCompNCAAM/data/raw/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dates(sdt, edt):\n",
    "    dates = [datetime.datetime.date(sdt)]\n",
    "    while sdt < edt:\n",
    "        sdt = sdt + datetime.timedelta(days=7)\n",
    "        dates.append(datetime.datetime.date(sdt))\n",
    "    return dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ranks(path):\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    cols = list(df.columns)\n",
    "    cols[4] = 'Delta'\n",
    "    df.columns = cols\n",
    "    to_remove = [cols[0], 'Conf', 'CMP', 'Sort', cols[-1]]\n",
    "    df.drop(to_remove, axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_ranks(df, week_num):\n",
    "    # Add the more repeating rank which indicates more popular rank\n",
    "    df['popularity_rank'] = df.apply(lambda x: mode(x[3:-2])[0][0], axis=1)\n",
    "    \n",
    "    # Drop various systems rankings, Just include Mean and StDev\n",
    "    df.drop(df.columns[3:-3], axis=1, inplace=True)\n",
    "    \n",
    "    # Converting W-L into wins and losses\n",
    "    # (# https://www.geeksforgeeks.org/split-a-text-column-into-two-columns-in-pandas-dataframe/)\n",
    "    df = pd.concat([df, pd.DataFrame(df['W-L'].str.split('-').to_list(), columns=['wins', 'losses'])], axis=1).drop('W-L', axis=1)\n",
    "    \n",
    "    # Convert the Delta to integer\n",
    "    df.Delta = df.Delta.fillna(0).astype('int')\n",
    "    \n",
    "    # Add the week number string to column names\n",
    "    cols = ['Team'] + list(map(lambda x: 'week{0}_'.format(week_num) + x, df.columns[1:]))\n",
    "    df.columns = cols\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_team_names(scraped_teams, cur_teams):\n",
    "    scraped_teams = list(ranks['Team'])\n",
    "    cur_teams = list(teams['TeamName'])\n",
    "    for each in scraped_teams:\n",
    "        assert each in cur_teams, \"Team `{0}` Does Not Exist\".format(each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 734,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_SEASONED_RANKS_PATH = DATA_DIR + 'MRanks/'\n",
    "if not os.path.exists(OUTPUT_SEASONED_RANKS_PATH):\n",
    "    os.mkdir(OUTPUT_SEASONED_RANKS_PATH)\n",
    "\n",
    "def write_season_ranks(sd, ed, season):\n",
    "    dates = get_dates(sd, ed)\n",
    "\n",
    "    dfs = []\n",
    "    for i, date in enumerate(dates):\n",
    "        path = OUTPUT_PATH + 'ranks_' + str(date).replace('-', '_') + '.csv'\n",
    "        df = read_ranks(path)\n",
    "        processed_df = process_ranks(df, i+1)\n",
    "        dfs.append(processed_df)\n",
    "\n",
    "    df_merged = reduce(lambda left, right: pd.merge(left, right, on='Team'), dfs)\n",
    "    \n",
    "    # Append the TeamID\n",
    "    try:\n",
    "        teams = pd.read_csv(DATA_DIR + 'MDataFiles_Stage1/MTeams.csv')\n",
    "        assert_team_names(df_merged['Team'], teams['TeamName'])\n",
    "        df_merged = df_merged.merge(teams[['TeamID', 'TeamName']], left_on='Team', right_on='TeamName').drop('TeamName', axis=1)\n",
    "    except AssertionError as e:\n",
    "        print(\"Continuing without TeamID for season {0} because {1}\".format(season, str(e)))\n",
    "        pass\n",
    "    \n",
    "    # Write to File\n",
    "    path = OUTPUT_SEASONED_RANKS_PATH + 'season_{0}'.format(season) + '.csv'\n",
    "    df_merged.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 739,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_season_ranks(datetime.datetime(2014, 11, 17), datetime.datetime(2015, 3, 16), '2015')\n",
    "write_season_ranks(datetime.datetime(2015, 11, 16), datetime.datetime(2016, 3, 14), '2016')\n",
    "write_season_ranks(datetime.datetime(2016, 11, 14), datetime.datetime(2017, 3, 13), '2017')\n",
    "write_season_ranks(datetime.datetime(2017, 11, 13), datetime.datetime(2018, 3, 12), '2018')\n",
    "write_season_ranks(datetime.datetime(2018, 11, 12), datetime.datetime(2019, 3, 18), '2019')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging Ranks with Events "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 740,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_processed_ranks_to_merge(season):\n",
    "    ranks = pd.read_csv(DATA_DIR + 'MRanks/season_{0}.csv'.format(season))\n",
    "    \n",
    "    # Pivot and stack to get the weekly attributes into rows\n",
    "    unp_ranks = ranks.pivot_table(index=['TeamID', 'Team']).stack().reset_index()\n",
    "    \n",
    "    # Get the week numbers and add the attributes as a new_col which can be further \n",
    "    unp_ranks['week'] = unp_ranks.level_2.apply(lambda x: re.findall(r'\\d+', x)[0])\n",
    "    unp_ranks[\"new_cols\"] = unp_ranks.level_2.str.split('_').apply(lambda x: \"_\".join(x[1:]))\n",
    "    \n",
    "    # Dropping and Renaming the columns \n",
    "    unp_ranks.drop(['level_2', 'Team'], axis=1, inplace=True)\n",
    "    unp_ranks.rename(columns={0: 'values'}, inplace=True)\n",
    "    \n",
    "    # Pivot using TeamID and week which will give all the attributes as the seperate columns\n",
    "    rank_pivots = unp_ranks.pivot_table(index=['TeamID', 'week'], columns=['new_cols'])\n",
    "    rank_pivots.columns = rank_pivots.columns.droplevel()\n",
    "    rank_pivots.reset_index(inplace=True)\n",
    "    \n",
    "    # Convert the TeamID and week to int.\n",
    "    rank_pivots['week'] = rank_pivots.week.astype('int')\n",
    "    rank_pivots['TeamID'] = rank_pivots.TeamID.astype('int')\n",
    "    \n",
    "    return rank_pivots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 741,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_events_for_merging(season):\n",
    "    events = pd.read_csv(DATA_DIR + 'HistoricalEvents/MEvents{0}.csv'.format(season))\n",
    "    events = events[['Season', 'DayNum', 'WTeamID', 'LTeamID']]\n",
    "    events['week_num'] = np.ceil(events['DayNum'] / 7).astype('int')\n",
    "    return events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 746,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_events_and_ranks(events, rank_pivots):\n",
    "    # First merge and get the winning teams attributes\n",
    "    events_with_ranks = pd.merge(events, \n",
    "                                 rank_pivots, \n",
    "                                 left_on=['week_num', 'WTeamID'], \n",
    "                                 right_on=['week', 'TeamID'], \n",
    "                                 how='left')\n",
    "    events_with_ranks.rename(columns={'Delta':'WDeltaRank', \n",
    "                                      'Mean': 'WMeanRank', \n",
    "                                      'StDev': 'WStDevRank', \n",
    "                                      'losses': 'WLosses', \n",
    "                                      'popularity_rank': 'WPopularityRank', \n",
    "                                      'wins': 'WWins'},                       \n",
    "                             inplace=True)\n",
    "    events_with_ranks = events_with_ranks.drop(['TeamID', 'week'], axis=1)\n",
    "    \n",
    "    # Repeat the process to get the loosing teams attributes\n",
    "    events_with_ranks = pd.merge(events_with_ranks, \n",
    "                                 rank_pivots, \n",
    "                                 left_on=['week_num', 'LTeamID'], \n",
    "                                 right_on=['week', 'TeamID'], \n",
    "                                 how='left')\n",
    "    events_with_ranks.rename(columns={'Delta':'LDeltaRank', \n",
    "                                      'Mean': 'LMeanRank', \n",
    "                                      'StDev': 'LStDevRank', \n",
    "                                      'losses': 'LLosses', \n",
    "                                      'popularity_rank': 'LPopularityRank', \n",
    "                                      'wins': 'LWins'},                       \n",
    "                             inplace=True)\n",
    "    events_with_ranks = events_with_ranks.drop(['TeamID', 'week', 'week_num'], axis=1)\n",
    "    \n",
    "    return events_with_ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 835,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vineethsuhas/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "OUTPUT_EVENT_RANKS_PATH = DATA_DIR + 'MEventsRanked/'\n",
    "if not os.path.exists(OUTPUT_EVENT_RANKS_PATH):\n",
    "    os.mkdir(OUTPUT_EVENT_RANKS_PATH)\n",
    "\n",
    "seasons = ['2015', '2016', '2017', '2018', '2019']\n",
    "\n",
    "for season in seasons:\n",
    "    ranks = get_processed_ranks_to_merge(season)\n",
    "    events = get_events_for_merging(season)\n",
    "    \n",
    "    # Fill the ranks for additional weeks in events with the latest rank\n",
    "    for i in range(0, (events.week_num.max() - ranks.week.max())):\n",
    "        add_week_data = ranks[ranks.week == ranks.week.max()]\n",
    "        add_week_data['week'] += 1\n",
    "        ranks = pd.concat([ranks, add_week_data])\n",
    "    \n",
    "    merged_events = merge_events_and_ranks(events, ranks)\n",
    "    \n",
    "    # Write to file:\n",
    "    file_path = OUTPUT_EVENT_RANKS_PATH + 'MEventsRanked{0}.csv'.format(season)\n",
    "    merged_events.to_csv(file_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
